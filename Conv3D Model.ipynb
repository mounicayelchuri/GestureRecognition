{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Recognition\n",
    "To build a 3D Conv model that will be able to predict the 5 gestures correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Import all necessary libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Load the training and testing data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('/datasets/Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('/datasets/Project_data/val.csv').readlines())\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = np.array([0, 48, 80], dtype = \"uint8\") ## lower limit for skin colour in HSV format\n",
    "upper = np.array([20, 255, 255], dtype = \"uint8\")## lower limit for skin colour in HSV format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Define generator function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [1,2,5,6,8,10,12,13,15,17,18,20,22,24,26,27,28,29]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size)\n",
    "        for batch in range(num_batches):\n",
    "            batch_data = np.zeros((batch_size,18,86,86,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])# loading image\n",
    "                    converted = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) # convert image to HSV format\n",
    "                    skinMask = cv2.inRange(converted, lower, upper) # setting boundaries to get the skin colour\n",
    "                    image = cv2.bitwise_and(image, image, mask = skinMask) # retaining only the region with skin colour\n",
    "                    \n",
    "                    if image.shape[1] == 160:\n",
    "                        image = resize(image[:,20:140,:],(86,86)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(86,86)).astype(np.float32)\n",
    "                    # normalize the image to get the range of data in between 0 to 255\n",
    "                    image = cv2.normalize(image, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F).astype(np.uint8) \n",
    "                    # bring each channel to mean\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 50 \n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 100\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 140\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n",
    "        if (len(t)%batch_size) != 0:\n",
    "            batch_data = np.zeros((len(t)%batch_size,18,86,86,3))\n",
    "            batch_labels = np.zeros((len(t)%batch_size,5))\n",
    "            for folder in range(len(t)%batch_size):\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    converted = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "                    skinMask = cv2.inRange(converted, lower, upper)\n",
    "                    image = cv2.bitwise_and(image, image, mask = skinMask)\n",
    "                    if image.shape[1] == 160:\n",
    "                        image = resize(image[:,20:140,:],(86,86)).astype(np.float32)\n",
    "                    else:\n",
    "                        image = resize(image,(86,86)).astype(np.float32)\n",
    "                    image = cv2.normalize(image, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F).astype(np.uint8)\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0] - 50\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1] - 100\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2] - 140\n",
    "\n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Define epochs,train and validation folder path__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = '/datasets/Project_data/train'\n",
    "val_path = '/datasets/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 30\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. create model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers,regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(18,84,84,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
    "\n",
    "model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. compile model with optimizers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_24 (Conv3D)           (None, 18, 84, 84, 64)    5248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 18, 84, 84, 64)    256       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 18, 84, 84, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 9, 42, 84, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_25 (Conv3D)           (None, 9, 42, 84, 128)    221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 9, 42, 84, 128)    512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 9, 42, 84, 128)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 4, 21, 42, 128)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 4, 21, 42, 256)    884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 4, 21, 42, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4, 21, 42, 256)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 2, 10, 21, 256)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 2, 10, 21, 256)    1769728   \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 2, 10, 21, 256)    1024      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2, 10, 21, 256)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 1, 5, 10, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_28 (Conv3D)           (None, 1, 5, 10, 256)     1769728   \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 1, 5, 10, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1, 5, 10, 256)     0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               1638528   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 6,294,021\n",
      "Trainable params: 6,292,101\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6.Defining callbacks__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Calculating number of steps per epoch in both train and validation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fitting the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  /datasets/Project_data/train ; batch size = 16\n",
      "Epoch 1/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 4.3029 - categorical_accuracy: 0.2896Source path =  /datasets/Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-07-3010_16_16.986489/model-00001-4.30287-0.28959-4.09462-0.18000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 4.3029 - categorical_accuracy: 0.2896 - val_loss: 4.0946 - val_categorical_accuracy: 0.1800\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.7757 - categorical_accuracy: 0.5113\n",
      "Epoch 00002: saving model to model_init_2021-07-3010_16_16.986489/model-00002-3.77571-0.51131-4.00088-0.38000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 3.7757 - categorical_accuracy: 0.5113 - val_loss: 4.0009 - val_categorical_accuracy: 0.3800\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.5426 - categorical_accuracy: 0.5867\n",
      "Epoch 00003: saving model to model_init_2021-07-3010_16_16.986489/model-00003-3.54262-0.58673-3.62787-0.58000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 3.5426 - categorical_accuracy: 0.5867 - val_loss: 3.6279 - val_categorical_accuracy: 0.5800\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.3453 - categorical_accuracy: 0.6637\n",
      "Epoch 00004: saving model to model_init_2021-07-3010_16_16.986489/model-00004-3.34534-0.66365-3.27378-0.74000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 3.3453 - categorical_accuracy: 0.6637 - val_loss: 3.2738 - val_categorical_accuracy: 0.7400\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.2398 - categorical_accuracy: 0.7315\n",
      "Epoch 00005: saving model to model_init_2021-07-3010_16_16.986489/model-00005-3.23979-0.73152-3.32943-0.69000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 3.2398 - categorical_accuracy: 0.7315 - val_loss: 3.3294 - val_categorical_accuracy: 0.6900\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.1417 - categorical_accuracy: 0.7481\n",
      "Epoch 00006: saving model to model_init_2021-07-3010_16_16.986489/model-00006-3.14169-0.74811-3.13367-0.76000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 3.1417 - categorical_accuracy: 0.7481 - val_loss: 3.1337 - val_categorical_accuracy: 0.7600\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.0179 - categorical_accuracy: 0.7919\n",
      "Epoch 00007: saving model to model_init_2021-07-3010_16_16.986489/model-00007-3.01790-0.79186-3.03617-0.76000.h5\n",
      "42/42 [==============================] - 91s 2s/step - loss: 3.0179 - categorical_accuracy: 0.7919 - val_loss: 3.0362 - val_categorical_accuracy: 0.7600\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.9874 - categorical_accuracy: 0.7994\n",
      "Epoch 00008: saving model to model_init_2021-07-3010_16_16.986489/model-00008-2.98739-0.79940-3.03421-0.75000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 2.9874 - categorical_accuracy: 0.7994 - val_loss: 3.0342 - val_categorical_accuracy: 0.7500\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.8691 - categorical_accuracy: 0.8296\n",
      "Epoch 00009: saving model to model_init_2021-07-3010_16_16.986489/model-00009-2.86908-0.82956-2.98438-0.84000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 2.8691 - categorical_accuracy: 0.8296 - val_loss: 2.9844 - val_categorical_accuracy: 0.8400\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.8576 - categorical_accuracy: 0.8311\n",
      "Epoch 00010: saving model to model_init_2021-07-3010_16_16.986489/model-00010-2.85759-0.83107-3.26281-0.70000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.8576 - categorical_accuracy: 0.8311 - val_loss: 3.2628 - val_categorical_accuracy: 0.7000\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.7967 - categorical_accuracy: 0.8416\n",
      "Epoch 00011: saving model to model_init_2021-07-3010_16_16.986489/model-00011-2.79666-0.84163-2.98137-0.81000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 2.7967 - categorical_accuracy: 0.8416 - val_loss: 2.9814 - val_categorical_accuracy: 0.8100\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.7003 - categorical_accuracy: 0.8929\n",
      "Epoch 00012: saving model to model_init_2021-07-3010_16_16.986489/model-00012-2.70025-0.89291-2.89658-0.85000.h5\n",
      "42/42 [==============================] - 98s 2s/step - loss: 2.7003 - categorical_accuracy: 0.8929 - val_loss: 2.8966 - val_categorical_accuracy: 0.8500\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.6933 - categorical_accuracy: 0.8824\n",
      "Epoch 00013: saving model to model_init_2021-07-3010_16_16.986489/model-00013-2.69330-0.88235-3.11087-0.66000.h5\n",
      "42/42 [==============================] - 95s 2s/step - loss: 2.6933 - categorical_accuracy: 0.8824 - val_loss: 3.1109 - val_categorical_accuracy: 0.6600\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.6093 - categorical_accuracy: 0.9080\n",
      "Epoch 00014: saving model to model_init_2021-07-3010_16_16.986489/model-00014-2.60928-0.90799-2.88277-0.80000.h5\n",
      "42/42 [==============================] - 91s 2s/step - loss: 2.6093 - categorical_accuracy: 0.9080 - val_loss: 2.8828 - val_categorical_accuracy: 0.8000\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.5965 - categorical_accuracy: 0.8944\n",
      "Epoch 00015: saving model to model_init_2021-07-3010_16_16.986489/model-00015-2.59654-0.89442-2.83942-0.84000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.5965 - categorical_accuracy: 0.8944 - val_loss: 2.8394 - val_categorical_accuracy: 0.8400\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.6072 - categorical_accuracy: 0.9005\n",
      "Epoch 00016: saving model to model_init_2021-07-3010_16_16.986489/model-00016-2.60721-0.90045-2.93453-0.76000.h5\n",
      "42/42 [==============================] - 97s 2s/step - loss: 2.6072 - categorical_accuracy: 0.9005 - val_loss: 2.9345 - val_categorical_accuracy: 0.7600\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.5449 - categorical_accuracy: 0.9351\n",
      "Epoch 00017: saving model to model_init_2021-07-3010_16_16.986489/model-00017-2.54488-0.93514-2.70398-0.87000.h5\n",
      "42/42 [==============================] - 98s 2s/step - loss: 2.5449 - categorical_accuracy: 0.9351 - val_loss: 2.7040 - val_categorical_accuracy: 0.8700\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.5119 - categorical_accuracy: 0.9201\n",
      "Epoch 00018: saving model to model_init_2021-07-3010_16_16.986489/model-00018-2.51189-0.92006-2.67645-0.88000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.5119 - categorical_accuracy: 0.9201 - val_loss: 2.6764 - val_categorical_accuracy: 0.8800\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.4512 - categorical_accuracy: 0.9457\n",
      "Epoch 00019: saving model to model_init_2021-07-3010_16_16.986489/model-00019-2.45121-0.94570-2.78926-0.85000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.4512 - categorical_accuracy: 0.9457 - val_loss: 2.7893 - val_categorical_accuracy: 0.8500\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.4472 - categorical_accuracy: 0.9517\n",
      "Epoch 00020: saving model to model_init_2021-07-3010_16_16.986489/model-00020-2.44724-0.95173-2.75243-0.87000.h5\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.4472 - categorical_accuracy: 0.9517 - val_loss: 2.7524 - val_categorical_accuracy: 0.8700\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3832 - categorical_accuracy: 0.9744\n",
      "Epoch 00021: saving model to model_init_2021-07-3010_16_16.986489/model-00021-2.38321-0.97436-2.74826-0.84000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 2.3832 - categorical_accuracy: 0.9744 - val_loss: 2.7483 - val_categorical_accuracy: 0.8400\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3696 - categorical_accuracy: 0.9683\n",
      "Epoch 00022: saving model to model_init_2021-07-3010_16_16.986489/model-00022-2.36965-0.96833-2.68569-0.87000.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "42/42 [==============================] - 93s 2s/step - loss: 2.3696 - categorical_accuracy: 0.9683 - val_loss: 2.6857 - val_categorical_accuracy: 0.8700\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3653 - categorical_accuracy: 0.9744\n",
      "Epoch 00023: saving model to model_init_2021-07-3010_16_16.986489/model-00023-2.36531-0.97436-2.62833-0.87000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.3653 - categorical_accuracy: 0.9744 - val_loss: 2.6283 - val_categorical_accuracy: 0.8700\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3555 - categorical_accuracy: 0.9789\n",
      "Epoch 00024: saving model to model_init_2021-07-3010_16_16.986489/model-00024-2.35545-0.97888-2.71806-0.91000.h5\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.3555 - categorical_accuracy: 0.9789 - val_loss: 2.7181 - val_categorical_accuracy: 0.9100\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3288 - categorical_accuracy: 0.9834\n",
      "Epoch 00025: saving model to model_init_2021-07-3010_16_16.986489/model-00025-2.32884-0.98341-2.66689-0.87000.h5\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "42/42 [==============================] - 95s 2s/step - loss: 2.3288 - categorical_accuracy: 0.9834 - val_loss: 2.6669 - val_categorical_accuracy: 0.8700\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3233 - categorical_accuracy: 0.9849\n",
      "Epoch 00026: saving model to model_init_2021-07-3010_16_16.986489/model-00026-2.32327-0.98492-2.71184-0.88000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 2.3233 - categorical_accuracy: 0.9849 - val_loss: 2.7118 - val_categorical_accuracy: 0.8800\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3281 - categorical_accuracy: 0.9804\n",
      "Epoch 00027: saving model to model_init_2021-07-3010_16_16.986489/model-00027-2.32809-0.98039-2.65305-0.88000.h5\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "42/42 [==============================] - 94s 2s/step - loss: 2.3281 - categorical_accuracy: 0.9804 - val_loss: 2.6531 - val_categorical_accuracy: 0.8800\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3368 - categorical_accuracy: 0.9774\n",
      "Epoch 00028: saving model to model_init_2021-07-3010_16_16.986489/model-00028-2.33677-0.97738-2.64458-0.89000.h5\n",
      "42/42 [==============================] - 91s 2s/step - loss: 2.3368 - categorical_accuracy: 0.9774 - val_loss: 2.6446 - val_categorical_accuracy: 0.8900\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3279 - categorical_accuracy: 0.9774\n",
      "Epoch 00029: saving model to model_init_2021-07-3010_16_16.986489/model-00029-2.32786-0.97738-2.67211-0.87000.h5\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "42/42 [==============================] - 92s 2s/step - loss: 2.3279 - categorical_accuracy: 0.9774 - val_loss: 2.6721 - val_categorical_accuracy: 0.8700\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.3172 - categorical_accuracy: 0.9834\n",
      "Epoch 00030: saving model to model_init_2021-07-3010_16_16.986489/model-00030-2.31723-0.98341-2.71488-0.85000.h5\n",
      "42/42 [==============================] - 91s 2s/step - loss: 2.3172 - categorical_accuracy: 0.9834 - val_loss: 2.7149 - val_categorical_accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5d4c9da20>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
